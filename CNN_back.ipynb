{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "data_file = open('images_zero_one.txt', 'rb')\n",
    "train_data, train_labels, test_data, test_labels = pickle.load(data_file, encoding='latin1')\n",
    "data_file.close()\n",
    "\n",
    "train_data = np.reshape(train_data, [-1, 28, 28])\n",
    "test_data = np.reshape(test_data, [-1, 28, 28])\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)\n",
    "\n",
    "train_data = (train_data - np.mean(train_data)) / np.std(train_data)\n",
    "test_data = (test_data - np.mean(test_data)) / np.std(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hm_train, dim1, dim2 = train_data.shape\n",
    "hm_test = len(test_labels)\n",
    "hm_classes = 2\n",
    "batch_size = 2\n",
    "max_epoch = 1\n",
    "eval_interval = 10\n",
    "learning_rate = 0.005\n",
    "test_acc = np.zeros((0, 1))\n",
    "train_acc = np.zeros((0, 1))\n",
    "train_loss = np.zeros((0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_init(shape):\n",
    "    initial = tf.truncated_normal(shape, mean=0.1, stddev=0.01) / 10\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_init(shape):\n",
    "    initial = tf.constant(0.001, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, w):\n",
    "    hm_data, dim1, dim2 = x.get_shape().as_list()\n",
    "    #print(hm_data, dim1, dim2)\n",
    "    stride = 1\n",
    "    filter_size1, filter_size2 = w.get_shape().as_list()\n",
    "    #print(filter_size1, filter_size2)\n",
    "    #convoled_im = tf.Variable(tf.ones([int(hm_data), int((dim1 - filter_size1) / stride + 1), int((dim2 - filter_size2) / stride + 1)]))\n",
    "    convoled_im = tf.zeros([0, 24, 24])\n",
    "    for im in np.arange(int(hm_data)): \n",
    "        slices = tf.zeros([0, 24])\n",
    "        for i in np.arange(int((dim1 - filter_size1) / stride + 1)):\n",
    "            row = tf.zeros([1, 0])\n",
    "            for j in np.arange(int((dim2 - filter_size2) / stride + 1)):\n",
    "                reception_area = x[im, i:i+filter_size1, j:j+filter_size2]\n",
    "                #print(reception_area.get_shape().as_list())\n",
    "                temp = tf.reshape(tf.reduce_sum(tf.reshape(reception_area, [filter_size1, filter_size2]) * w), [1, 1])\n",
    "                row = tf.concat([row, temp], 1)\n",
    "                #print(temp.get_shape().as_list())\n",
    "                #convoled_im[im, i, j].assign(temp)\n",
    "            slices = tf.concat([slices, row], 0)\n",
    "        convoled_im = tf.concat([convoled_im, slices[None, :, :]], 0)\n",
    "    return tf.reshape(convoled_im, [-1, 24, 24])\n",
    "\n",
    "def avg_pool(x):\n",
    "    hm_data, dim1, dim2 = x.get_shape().as_list()\n",
    "    print(hm_data, dim1, dim2)\n",
    "    stride = 2\n",
    "    kernel_size = 2\n",
    "    #pooled_im = tf.Variable(tf.zeros([int(hm_data), (dim1 - kernel_size) / stride + 1, (dim2 - kernel_size) / stride + 1]), trainable=False)\n",
    "    pooled_im = tf.zeros([0, 12, 12])\n",
    "    for im in np.arange(hm_data):\n",
    "        slices = tf.zeros([0, 12])\n",
    "        for i in np.arange(int((dim1 - kernel_size) / stride + 1)):\n",
    "            row = tf.zeros([1, 0])\n",
    "            for j in np.arange(int((dim2 - kernel_size) / stride + 1)):\n",
    "                reception_area = x[im, i * stride:i * stride + kernel_size, j * stride:j * stride+kernel_size]\n",
    "                temp = tf.reshape(tf.reduce_mean(reception_area), [1, 1])\n",
    "                row = tf.concat([row, temp], 1)\n",
    "                \n",
    "            slices = tf.concat([slices, row], 0)\n",
    "        pooled_im = tf.concat([pooled_im, slices[None, :, :]], 0)\n",
    "            \n",
    "    return pooled_im\n",
    "\n",
    "def sigmoid_cross_entropy(y_pred, y):\n",
    "    return tf.matmul(-tf.transpose(y), tf.log(tf.sigmoid(y_pred))) - tf.matmul(tf.transpose(1-y), tf.log(1 - tf.sigmoid(y_pred)))\n",
    "\n",
    "def compute_gradient(fc, y_out, w_fc, b_fc, x, y, h_conv1, h_conv2):\n",
    "    grad_w_fc = tf.matmul(tf.transpose(fc), (1 - tf.sigmoid(y_out)) * -y) - tf.matmul(tf.transpose(fc), (1-y) * tf.sigmoid(y_out))\n",
    "    grad_b_fc = tf.reshape(tf.reduce_sum(1 - tf.sigmoid(y_out) * -y - (1-y) * tf.sigmoid(y_out)), [1, 1])\n",
    "    grad_fc = tf.matmul(-y * (1 - tf.sigmoid(y_out)), tf.transpose(w_fc)) - tf.matmul((1-y) * tf.sigmoid(y_out), tf.transpose(w_fc))\n",
    "    #print(grad_fc.get_shape().as_list())\n",
    "    #activation1 = tf.zeros([0, 24, 24])\n",
    "    #activation2 = tf.zeros([0, 24, 24])\n",
    "    grad_w1 = tf.zeros([5, 5])\n",
    "    grad_w2 = tf.zeros([5, 5])\n",
    "    grad_b1 = tf.zeros([24, 24])\n",
    "    grad_b2 = tf.zeros([24, 24])\n",
    "    for im in np.arange(batch_size):\n",
    "        one_im = tf.reshape(grad_fc[im, :], [288, 1])\n",
    "        # print(one_im.get_shape().as_list())\n",
    "        pool1 = tf.reshape(one_im[0:144, 0], [12, 12])\n",
    "        pool2 = tf.reshape(one_im[144:, 0], [12, 12])\n",
    "        act1 = tf.zeros([0, 24])\n",
    "        act2 = tf.zeros([0, 24])\n",
    "        for i in np.arange(12):\n",
    "            act_r1 = tf.zeros([2, 0])\n",
    "            act_r2 = tf.zeros([2, 0])\n",
    "            for j in np.arange(12):\n",
    "                temp1 = tf.ones([2, 2]) * pool1[i, j] / 4\n",
    "                temp2 = tf.ones([2, 2]) * pool2[i, j] / 4\n",
    "                act_r1 = tf.concat([act_r1, temp1], 1)\n",
    "                act_r2 = tf.concat([act_r2, temp2], 1)\n",
    "                \n",
    "            act1 = tf.concat([act1, act_r1], 0)\n",
    "            act2 = tf.concat([act2, act_r2], 0)\n",
    "            \n",
    "        #activation1 = tf.concat([activation1, act1[None, :, :]], 0)\n",
    "        #activation2 = tf.concat([activation2, act2[None, :, :]], 0)\n",
    "        print(grad_b1.get_shape().as_list())\n",
    "        grad_C1 = tf.cast(h_conv1[im, :, :] > 0, tf.float32) * act1[None, :, :]\n",
    "        grad_C2 = tf.cast(h_conv2[im, :, :] > 0, tf.float32) * act2[None, :, :]\n",
    "        grad_b1 += tf.reshape(grad_C1, [24, 24])\n",
    "        grad_b2 += tf.reshape(grad_C2, [24, 24])\n",
    "        print(grad_C1.get_shape().as_list())\n",
    "        print(grad_b1.get_shape().as_list())\n",
    "        f1 = tf.zeros([0, 5])\n",
    "        f2 = tf.zeros([0, 5])\n",
    "        for r in np.arange(5):\n",
    "            t1 = tf.zeros([1, 0])\n",
    "            t2 = tf.zeros([1, 0])\n",
    "            for c in np.arange(5):\n",
    "                t1 = tf.concat([t1, tf.reshape(tf.reduce_sum(x[im, r:r+24, c:c+24] * grad_C1), [1, 1])], 1)\n",
    "                t2 = tf.concat([t2, tf.reshape(tf.reduce_sum(x[im, r:r+24, c:c+24] * grad_C2), [1, 1])], 1)\n",
    "            \n",
    "            f1 = tf.concat([f1, t1], 0)\n",
    "            f2 = tf.concat([f2, t2], 0)\n",
    "            \n",
    "        grad_w1 += f1\n",
    "        grad_w2 += f2\n",
    "        \n",
    "    return grad_w_fc, grad_b_fc, grad_w1, grad_b1, grad_w2, grad_b2\n",
    "\n",
    "def train_opt(grad_w_fc, grad_b_fc, grad_w1, grad_b1, grad_w2, grad_b2, w_fc, b_fc, w1, b1, w2, b2, rate):\n",
    "    tf.assign_add(w_fc, -rate * grad_w_fc)\n",
    "    print(b1.get_shape().as_list())\n",
    "    print(grad_b1.get_shape().as_list())\n",
    "    \n",
    "    tf.assign_add(b_fc, -rate * grad_b_fc[:, 0])\n",
    "    tf.assign_add(w1, -rate * grad_w1)\n",
    "    tf.assign_add(b1, -rate * grad_b1)\n",
    "    tf.assign_add(w2, -rate * grad_w2)\n",
    "    tf.assign_add(b2, -rate * grad_b2)\n",
    "    \n",
    "    return w_fc, b_fc, w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the flow\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[batch_size, 28, 28])\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[batch_size, 1])\n",
    "\n",
    "w1 = weight_init([5, 5])\n",
    "b1 = bias_init([24, 24])\n",
    "\n",
    "w2 = weight_init([5, 5])\n",
    "b2 = bias_init([24, 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 24 24\n",
      "2 24 24\n"
     ]
    }
   ],
   "source": [
    "# Convolutional layer, and pooling with two filters\n",
    "h_conv1 = tf.nn.relu(conv2d(x, w1) + b1)\n",
    "h_pool1 = avg_pool(h_conv1)\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(x, w2) + b2)\n",
    "h_pool2 = avg_pool(h_conv2)\n",
    "\n",
    "# fully connected layer\n",
    "fc = tf.concat([tf.reshape(h_pool1, [-1, 144]), tf.reshape(h_pool2, [-1, 144])], 1)\n",
    "w_fc = weight_init([144 * 2, 1])\n",
    "b_fc = bias_init([1])\n",
    "\n",
    "# output layer\n",
    "y_out = tf.matmul(fc, w_fc) + b_fc\n",
    "y_pred = tf.sigmoid(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 24]\n",
      "[1, 24, 24]\n",
      "[24, 24]\n",
      "[24, 24]\n",
      "[1, 24, 24]\n",
      "[24, 24]\n"
     ]
    }
   ],
   "source": [
    "# The backward propagation\n",
    "# define the loss function\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred, labels=y))\n",
    "cross_entropy = sigmoid_cross_entropy(y_out, y)\n",
    "G_w_fc, G_b_fc, G_w1, G_b1, G_w2, G_b2 = compute_gradient(fc, y_out, w_fc, b_fc, x, y, h_conv1, h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 24]\n",
      "[24, 24]\n"
     ]
    }
   ],
   "source": [
    "a = train_opt(G_w_fc, G_b_fc, G_w1, G_b1, G_w2, G_b2, w_fc, b_fc, w1, b1, w2, b2, learning_rate)\n",
    "\n",
    "correct_prediction = tf.equal(tf.to_float(y_pred >= 0.5), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "for epoch in np.arange(max_epoch):\n",
    "    re_order = np.random.permutation(hm_train)\n",
    "    \n",
    "        \n",
    "    for i in np.arange(int(hm_train / batch_size)):\n",
    "        batch_x = train_data[re_order[i * batch_size:(i + 1) * batch_size], :, :]\n",
    "        batch_y = train_labels[re_order[i * batch_size:(i + 1) * batch_size]][:, None]\n",
    "            \n",
    "        ce = sess.run(cross_entropy, feed_dict={x: batch_x, y: batch_y})\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "        #test_accuracy = sess.run(accuracy, feed_dict={x: test_data, y: test_labels[:, None]})\n",
    "        train_acc = np.vstack((train_acc, train_accuracy))\n",
    "        train_loss = np.vstack((train_loss, ce))\n",
    "        #test_acc = np.vstack((test_acc, test_accuracy))\n",
    "            \n",
    "        if i % eval_interval == 0:\n",
    "            learning_rate *= 0.9\n",
    "            print(\"epoch: %d step: %d, training accuracy: %g, loss: %g\" %\n",
    "                            (epoch, i, np.mean(train_acc), np.mean(train_loss)))\n",
    "                \n",
    "        \n",
    "                \n",
    "        sess.run(a, feed_dict={x: batch_x, y:batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(int(1100 / batch_size)):\n",
    "    batch_x = test_data[i * batch_size:(i + 1) * batch_size, :, :]\n",
    "    batch_y = test_labels[i * batch_size:(i + 1) * batch_size][:, None]\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "    test_acc = np.vstack((train_acc, train_accuracy))\n",
    "            \n",
    "print(\"testing accuracy: %g \" % np.mean(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "for epoch in np.arange(1):\n",
    "    \n",
    "    batch_x = train_data[0:batch_size, :, :]\n",
    "    batch_y = train_labels[0:batch_size][:, None]\n",
    "            \n",
    "    ce = sess.run(cross_entropy, feed_dict={x: batch_x, y: batch_y})\n",
    "    Grad_w_fc, _, Grad_w1, _, _, _ = sess.run(compute_gradient(fc, y_out, w_fc, b_fc, x, y, h_conv1, h_conv2), feed_dict={x: batch_x, y: batch_y})\n",
    "    compare_wfc = sess.run(tf.gradients(cross_entropy, w_fc), feed_dict={x: batch_x, y: batch_y})\n",
    "    compare_w1 = sess.run(tf.gradients(cross_entropy, w1), feed_dict={x: batch_x, y: batch_y})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.0004151 ]\n",
      " [-0.00891668]\n",
      " [-0.00930318]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.02336233]\n",
      " [-0.13094537]\n",
      " [-0.11931626]\n",
      " [-0.00896868]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.07830977]\n",
      " [-0.2757951 ]\n",
      " [-0.23357067]\n",
      " [-0.02556819]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.14110583]\n",
      " [-0.32920167]\n",
      " [-0.2387878 ]\n",
      " [-0.02291412]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.00116262]\n",
      " [-0.21253577]\n",
      " [-0.35589141]\n",
      " [-0.20068158]\n",
      " [-0.00885565]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.0191385 ]\n",
      " [-0.26550195]\n",
      " [-0.35935572]\n",
      " [-0.14954865]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.05434499]\n",
      " [-0.30931693]\n",
      " [-0.3338598 ]\n",
      " [-0.08379674]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.11829716]\n",
      " [-0.34221315]\n",
      " [-0.27931917]\n",
      " [-0.0337444 ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.00700007]\n",
      " [-0.20448941]\n",
      " [-0.36498302]\n",
      " [-0.21399364]\n",
      " [-0.00194377]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.02390288]\n",
      " [-0.26489002]\n",
      " [-0.38419783]\n",
      " [-0.17891014]\n",
      " [-0.00314048]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.01109381]\n",
      " [-0.16678607]\n",
      " [-0.23731068]\n",
      " [-0.09257224]\n",
      " [-0.00086107]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.02283175]\n",
      " [-0.03962467]\n",
      " [-0.01166159]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.00424363]\n",
      " [-0.00567089]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.02219987]\n",
      " [-0.11835951]\n",
      " [-0.10890939]\n",
      " [-0.00921608]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.07584462]\n",
      " [-0.25994512]\n",
      " [-0.2203705 ]\n",
      " [-0.02518911]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.13681264]\n",
      " [-0.31120253]\n",
      " [-0.22755323]\n",
      " [-0.0227472 ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.00082525]\n",
      " [-0.20617993]\n",
      " [-0.33501607]\n",
      " [-0.19309583]\n",
      " [-0.00930014]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.01810319]\n",
      " [-0.25684908]\n",
      " [-0.33821058]\n",
      " [-0.14549416]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.05205474]\n",
      " [-0.29741788]\n",
      " [-0.31505364]\n",
      " [-0.08325192]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.11483742]\n",
      " [-0.32629192]\n",
      " [-0.26650941]\n",
      " [-0.03409781]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.00607686]\n",
      " [-0.19671407]\n",
      " [-0.34527069]\n",
      " [-0.20501855]\n",
      " [-0.00174583]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.02331513]\n",
      " [-0.25636449]\n",
      " [-0.36218023]\n",
      " [-0.16989315]\n",
      " [-0.00303202]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.01512485]\n",
      " [-0.17183051]\n",
      " [-0.23385319]\n",
      " [-0.0922105 ]\n",
      " [-0.0020019 ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.02573662]\n",
      " [-0.03974268]\n",
      " [-0.01122913]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(Grad_w_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.0004151 ],\n",
      "       [-0.00891668],\n",
      "       [-0.00930318],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.02336233],\n",
      "       [-0.13094537],\n",
      "       [-0.11931626],\n",
      "       [-0.00896868],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.07830977],\n",
      "       [-0.2757951 ],\n",
      "       [-0.23357067],\n",
      "       [-0.02556819],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.14110583],\n",
      "       [-0.32920167],\n",
      "       [-0.2387878 ],\n",
      "       [-0.02291412],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.00116262],\n",
      "       [-0.21253577],\n",
      "       [-0.35589141],\n",
      "       [-0.20068158],\n",
      "       [-0.00885565],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.0191385 ],\n",
      "       [-0.26550195],\n",
      "       [-0.35935572],\n",
      "       [-0.14954865],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.05434499],\n",
      "       [-0.30931693],\n",
      "       [-0.3338598 ],\n",
      "       [-0.08379674],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.11829716],\n",
      "       [-0.34221315],\n",
      "       [-0.27931917],\n",
      "       [-0.0337444 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.00700007],\n",
      "       [-0.20448941],\n",
      "       [-0.36498302],\n",
      "       [-0.21399364],\n",
      "       [-0.00194377],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.02390288],\n",
      "       [-0.26489002],\n",
      "       [-0.38419783],\n",
      "       [-0.17891014],\n",
      "       [-0.00314048],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.01109381],\n",
      "       [-0.16678607],\n",
      "       [-0.23731068],\n",
      "       [-0.09257224],\n",
      "       [-0.00086107],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.02283175],\n",
      "       [-0.03962467],\n",
      "       [-0.01166159],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.00424363],\n",
      "       [-0.00567089],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.02219987],\n",
      "       [-0.11835951],\n",
      "       [-0.10890939],\n",
      "       [-0.00921608],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.07584462],\n",
      "       [-0.25994512],\n",
      "       [-0.2203705 ],\n",
      "       [-0.02518911],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.13681264],\n",
      "       [-0.31120253],\n",
      "       [-0.22755323],\n",
      "       [-0.0227472 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.00082525],\n",
      "       [-0.20617993],\n",
      "       [-0.33501607],\n",
      "       [-0.19309583],\n",
      "       [-0.00930014],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.01810319],\n",
      "       [-0.25684908],\n",
      "       [-0.33821058],\n",
      "       [-0.14549416],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.05205474],\n",
      "       [-0.29741788],\n",
      "       [-0.31505364],\n",
      "       [-0.08325192],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.11483742],\n",
      "       [-0.32629192],\n",
      "       [-0.26650941],\n",
      "       [-0.03409781],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.00607686],\n",
      "       [-0.19671407],\n",
      "       [-0.34527069],\n",
      "       [-0.20501855],\n",
      "       [-0.00174583],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.02331513],\n",
      "       [-0.25636449],\n",
      "       [-0.36218023],\n",
      "       [-0.16989315],\n",
      "       [-0.00303202],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.01512485],\n",
      "       [-0.17183051],\n",
      "       [-0.23385319],\n",
      "       [-0.0922105 ],\n",
      "       [-0.0020019 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.02573662],\n",
      "       [-0.03974268],\n",
      "       [-0.01122913],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(compare_wfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.19678211 -0.26824868 -0.28050941 -0.27819446 -0.25289649]\n",
      " [-0.23463902 -0.29250535 -0.29847866 -0.29639342 -0.26399124]\n",
      " [-0.25376344 -0.29285061 -0.29843727 -0.29766166 -0.25829631]\n",
      " [-0.2593267  -0.29288825 -0.29801801 -0.29560554 -0.23564357]\n",
      " [-0.25277612 -0.28130531 -0.28598326 -0.2778092  -0.19716889]]\n"
     ]
    }
   ],
   "source": [
    "print(Grad_w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.19678208, -0.26824859, -0.28050935, -0.27819449, -0.25289646],\n",
      "       [-0.23463903, -0.29250523, -0.29847866, -0.29639348, -0.26399121],\n",
      "       [-0.2537635 , -0.29285058, -0.29843724, -0.2976616 , -0.25829634],\n",
      "       [-0.25932673, -0.29288822, -0.29801801, -0.29560551, -0.23564357],\n",
      "       [-0.25277615, -0.28130528, -0.28598326, -0.2778092 , -0.19716889]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(compare_w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
