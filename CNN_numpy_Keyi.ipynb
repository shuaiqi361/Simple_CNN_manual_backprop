{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data_file = open('images_zero_one.txt', 'rb')\n",
    "train_data, train_labels, test_data, test_labels = pickle.load(data_file, encoding='latin1')\n",
    "data_file.close()\n",
    "\n",
    "train_data = np.reshape(train_data, [-1, 28, 28])\n",
    "test_data = np.reshape(test_data, [-1, 28, 28])\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)\n",
    "\n",
    "# normalize the data, note that the testing data has to be manipulated in the same way as the training data\n",
    "train_data = (train_data - np.mean(train_data)) / np.std(train_data)\n",
    "test_data = (test_data - np.mean(train_data)) / np.std(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1623,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABTxJREFUeJzt3TmInWUUgOGZyUKMRhAlCSYhoETTWNi4NiIGJAhWCmor\nuBTaaGFvKygGCxHEOopioYKFlSFioSiCC0FxF0XBwkTNzLVX7jch453tfZ723H+B4Z2vOMnc+clk\nMgf0LKz1CwBrQ/wQJX6IEj9EiR+ixA9R4oco8UOU+CFq62o+7MjCXf45IczY20vH58/lc05+iBI/\nRIkfosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco8UOU+CFK/BAlfogSP0SJH6LED1Hihyjx\nQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFD1Kp+RTebz6mnbhjOH739zamzN+65cXjt0kefntc7cW6c\n/BAlfogSP0SJH6LED1HihyjxQ5Q9P0Nb910+nB+788Xh/MgFp6fOXrr+6PDaSz8ajlkhJz9EiR+i\nxA9R4oco8UOU+CFK/BBlz8/QqQcODuejPT7rm5MfosQPUeKHKPFDlPghSvwQZdXH0IGbv13rV2BG\nnPwQJX6IEj9EiR+ixA9R4oco8UOUPX/cmTuuG86fueLZZe6w7f97GVaVkx+ixA9R4oco8UOU+CFK\n/BAlfoiy5487femW4fya7fb4m5WTH6LED1HihyjxQ5T4IUr8ECV+iLLnZ6be/XP6+bLrm7Or+Cb8\nm5MfosQPUeKHKPFDlPghSvwQJX6IsuePO/zgJzO9/9PfHpk62/7W+zN9NmNOfogSP0SJH6LED1Hi\nhyjxQ5RVX9zDe95Z5hPzK7r/Z28emjrbP/fziu7Nyjj5IUr8ECV+iBI/RIkfosQPUeKHKHt+Zurg\na9N3+Yur+B78l5MfosQPUeKHKPFDlPghSvwQJX6Isuff5H565Kbh/OptJ5a5w47h9LvFP8aXn7XN\nX6+c/BAlfogSP0SJH6LED1HihyjxQ5Q9/yawZc/uqbNr7/14eO3FC+M9/nJuefWx4fzQFydXdH9m\nx8kPUeKHKPFDlPghSvwQJX6IEj9E2fNvBpddMnX0woG3VnTr35fODOe7vnR+bFR+chAlfogSP0SJ\nH6LED1Hihyirvk1g8cLtM7v3x3/vHM73Pr3cn/5mvXLyQ5T4IUr8ECV+iBI/RIkfosQPUfb8m8Cu\np36Y2b0f+uC+4Xz/3Cczezaz5eSHKPFDlPghSvwQJX6IEj9EiR+i7Pk3gK0H9g/nV1309Xnf+76v\nbhvOD97//XC+eN5PZq05+SFK/BAlfogSP0SJH6LED1Hihyh7/g3gx6MHhvPXd78+dbZlfvz7/bcz\n47/Lv/DXb8P5/LbxdwZM/v5rOGftOPkhSvwQJX6IEj9EiR+ixA9RVn2b3OJkaTh/4/D0NeHc3Nzc\n3Ofj8aGXHx7PHz05vgFrxskPUeKHKPFDlPghSvwQJX6IEj9E2fNvADt+He/qT509PXV25dYLVvTs\n05Pxf8nd+YPzY6Pyk4Mo8UOU+CFK/BAlfogSP0SJH6Ls+TeAi46/N5zfvffxqbMPn3hueO2Tvxwe\nzl95/tbhfN+xE8M565eTH6LED1HihyjxQ5T4IUr8ECV+iJqfTCar9rAjC3et3sMg6u2l4/Pn8jkn\nP0SJH6LED1HihyjxQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco\n8UOU+CFK/BAlfogSP0SJH6LED1HihyjxQ5T4IWpVv6IbWD+c/BAlfogSP0SJH6LED1HihyjxQ5T4\nIUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco8UPUP1M2eJB6MvCzAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111514630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the image\n",
    "plt.figure(1)\n",
    "img = train_data[1, :, :].reshape(28,28)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hm_train, dim1, dim2 = train_data.shape\n",
    "hm_test = len(test_labels)\n",
    "hm_classes = 2\n",
    "batch_size = 2\n",
    "max_epoch = 1\n",
    "eval_interval = 20\n",
    "learning_rate = 0.0001\n",
    "eps = 1e-8\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "test_acc = np.zeros((0, 1))\n",
    "train_acc = np.zeros((0, 1))\n",
    "train_loss = np.zeros((0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weight(shape):\n",
    "    w = np.ones(shape) / 100\n",
    "    #w = np.abs(np.random.randn(shape[0], shape[1])) / 100\n",
    "    return w\n",
    "\n",
    "\n",
    "def init_bias(shape):\n",
    "    b = np.ones(shape) * 0.001\n",
    "    return b\n",
    "\n",
    "\n",
    "def sigmoid_act(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "def sigmoid_grad(x):\n",
    "    return sigmoid_act(x) * (1 - sigmoid_act(x))\n",
    "\n",
    "\n",
    "def Relu_act(x):\n",
    "    return (1 + np.sign(x)) / 2 * x\n",
    "\n",
    "\n",
    "def Relu_grad(x):\n",
    "    return (1 + np.sign(x)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_np(images, filt2d):\n",
    "    hm_im, d1, d2 = images.shape\n",
    "    \n",
    "    # default stride 1 and square filter with size 5\n",
    "    stride = 1\n",
    "    filter_size = 5\n",
    "    conv_im = np.zeros([hm_im, int((d1-filter_size)/stride + 1), int((d2-filter_size)/stride + 1)])\n",
    "    \n",
    "    # convolve with the input images\n",
    "    for im in np.arange(int(hm_im)):\n",
    "        for i in np.arange(int((d1-filter_size+1)/stride)):\n",
    "            for j in np.arange(int((d2-filter_size+1)/stride)):\n",
    "                recep_area = images[im, i:i+filter_size, j:j+filter_size]\n",
    "                conv_im[im, i, j] = np.sum(recep_area * filt2d)\n",
    "                \n",
    "    return conv_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_pool(images):\n",
    "    hm_im, d1, d2 = images.shape\n",
    "    # by default, the stride is 2, and the kernel is 2x2\n",
    "    stride = 2\n",
    "    kernel_size = 2\n",
    "    pooled_im = np.zeros([hm_im, int((d1-kernel_size)/stride + 1), int((d2-kernel_size)/stride + 1)])\n",
    "    for im in np.arange(int(hm_im)):\n",
    "        for i in np.arange(int((d1-kernel_size)/stride + 1)):\n",
    "            for j in np.arange(int((d2-kernel_size)/stride + 1)):\n",
    "                recep_area = images[im, i * stride:i * stride + kernel_size, j * stride:j * stride + kernel_size]\n",
    "                pooled_im[im, i, j] = np.mean(recep_area)\n",
    "                \n",
    "    return pooled_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_image = train_data[0:2, :, :]\n",
    "test_label = train_labels[0:2][:, None]\n",
    "\n",
    "W = {'w1': init_weight([5, 5]), 'b1': init_bias([24, 24]), \\\n",
    "    'w2': init_weight([5, 5]), 'b2': init_bias([24, 24]), \\\n",
    "    'w_fc': init_weight([144 * 2, 1]), 'b_fc': init_bias([1, 1])}\n",
    "\n",
    "h_conv1 = Relu_act(conv2d_np(test_image, W['w1']) + W['b1'])\n",
    "h_pool1 = avg_pool(h_conv1)\n",
    "\n",
    "h_conv2 = Relu_act(conv2d_np(test_image, W['w2']) + W['b2'])\n",
    "h_pool2 = avg_pool(h_conv2)\n",
    "\n",
    "# feed into the fully connected layer\n",
    "fc = np.hstack((h_pool1.reshape(-1, 144), h_pool2.reshape(-1, 144)))\n",
    "y_out = np.dot(fc, W['w_fc']) + W['b_fc']\n",
    "y_pred = sigmoid_act(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(data, labels, W):\n",
    "    h_conv1 = Relu_act(conv2d_np(data, W['w1']) + W['b1'])\n",
    "    h_pool1 = avg_pool(h_conv1)\n",
    "    \n",
    "    h_conv2 = Relu_act(conv2d_np(data, W['w2']) + W['b2'])\n",
    "    h_pool2 = avg_pool(h_conv2)\n",
    "    \n",
    "    # feed into the fully connected layer\n",
    "    fc = np.hstack((h_pool1.reshape(-1, 144), h_pool2.reshape(-1, 144)))\n",
    "    y_out = np.dot(fc, W['w_fc']) + W['b_fc']\n",
    "    \n",
    "    pred_labels = (y_out > 0) * 1.\n",
    "    #print(pred_labels.shape)\n",
    "    #print(labels.shape)\n",
    "    acc = np.sum(np.equal(pred_labels, labels) * 1.0) / len(labels)\n",
    "    #print(np.equal(pred_labels, labels).shape)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_ce_sigmoid_loss(y_hat, y):\n",
    "    return np.sum(y * -np.log(y_hat) + (1-y) * -np.log(1-y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the gadients\n",
    "Grad_W = {'w1': np.zeros([5, 5]), 'b1': np.zeros([24, 24]), \\\n",
    "    'w2': np.zeros([5, 5]), 'b2': np.zeros([24, 24]), \\\n",
    "    'w_fc': np.zeros([144 * 2, 1]), 'b_fc': np.zeros([1, 1])}\n",
    "\n",
    "m_t = {'w1': np.zeros([5, 5]), 'b1': np.zeros([24, 24]), \\\n",
    "    'w2': np.zeros([5, 5]), 'b2': np.zeros([24, 24]), \\\n",
    "    'w_fc': np.zeros([144 * 2, 1]), 'b_fc': np.zeros([1, 1])}\n",
    "\n",
    "cross_entropy_sigmoid = compute_ce_sigmoid_loss(y_pred, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradients(W, x, h_conv1, h_conv2, fc, y, y_hat):\n",
    "    # initialize the gradients to return\n",
    "    Grad_W = {'w1': np.zeros([5, 5]), 'b1': np.zeros([24, 24]), \\\n",
    "    'w2': np.zeros([5, 5]), 'b2': np.zeros([24, 24]), \\\n",
    "    'w_fc': np.zeros([144 * 2, 1]), 'b_fc': np.zeros([1, 1])}\n",
    "    \n",
    "    Grad_W['w_fc'] = np.dot(np.transpose(fc), 2*y*y_hat - y - y_hat)\n",
    "    Grad_W['b_fc'] = np.sum(2*y*y_hat - y - y_hat)\n",
    "    Grad_fc = np.dot(2*y*y_hat - y - y_hat, np.transpose(W['w_fc']))\n",
    "    \n",
    "    act1 = np.zeros([24, 24])\n",
    "    act2 = np.zeros([24, 24])\n",
    "    for im in np.arange(len(y)):\n",
    "        one_im = Grad_fc[im, :]\n",
    "        pool1 = one_im[0:144].reshape(12, 12)\n",
    "        pool2 = one_im[144:].reshape(12, 12)\n",
    "        for i in np.arange(12):\n",
    "            for j in np.arange(12):\n",
    "                act1[i*2:i*2+2, j*2:j*2+2] = np.ones([2, 2]) * pool1[i, j] / 4\n",
    "                act2[i*2:i*2+2, j*2:j*2+2] = np.ones([2, 2]) * pool2[i, j] / 4\n",
    "                \n",
    "        Grad_C1 = (h_conv1[im, :, :].reshape(24, 24) > 0) * act1\n",
    "        Grad_C2 = (h_conv2[im, :, :].reshape(24, 24) > 0) * act2\n",
    "        \n",
    "        Grad_W['b1'] += Grad_C1\n",
    "        Grad_W['b2'] += Grad_C2\n",
    "        \n",
    "        f1 = np.zeros([5, 5])\n",
    "        f2 = np.zeros([5, 5])\n",
    "        for r in np.arange(5):\n",
    "            for c in np.arange(5):\n",
    "                f1[r, c] = np.sum(x[im, r:r+24, c:c+24].reshape(24, 24) * Grad_C1)\n",
    "                f2[r, c] = np.sum(x[im, r:r+24, c:c+24].reshape(24, 24) * Grad_C2)\n",
    "                \n",
    "        Grad_W['w1'] += f1\n",
    "        Grad_W['w2'] += f2\n",
    "        \n",
    "    return Grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Grad_W = compute_gradients(W, test_image, h_conv1, h_conv2, fc, test_label, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.20380894 -0.27027063 -0.2770126  -0.27396227 -0.2552794 ]\n",
      " [-0.24177034 -0.29419433 -0.29442414 -0.29264946 -0.26741981]\n",
      " [-0.26012179 -0.29442414 -0.29442414 -0.29442414 -0.25897422]\n",
      " [-0.26416191 -0.29315378 -0.29442414 -0.293476   -0.23489117]\n",
      " [-0.25489307 -0.27934354 -0.28198175 -0.27645617 -0.19596766]]\n"
     ]
    }
   ],
   "source": [
    "print(Grad_W['w1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_opts(W, Grad_W, Cum_Grad_W, learning_rate):\n",
    "    \n",
    "    m_t['w1'] = m_t['w1'] * beta1 + (1 - beta1) * Grad_W['w1']\n",
    "    W['w1'] -= learning_rate * m_t['w1'] / np.sqrt(Cum_Grad_W['w1'] + eps)\n",
    "    m_t['b1'] = m_t['b1'] * beta1 + (1 - beta1) * Grad_W['b1']\n",
    "    W['b1'] -= learning_rate * m_t['b1'] / np.sqrt(Cum_Grad_W['b1'] + eps)\n",
    "    m_t['w2'] = m_t['w2'] * beta1 + (1 - beta1) * Grad_W['w2']\n",
    "    W['w2'] -= learning_rate * m_t['w2'] / np.sqrt(Cum_Grad_W['w2'] + eps)\n",
    "    m_t['b2'] = m_t['b2'] * beta1 + (1 - beta1) * Grad_W['b2']\n",
    "    W['b2'] -= learning_rate * m_t['b2'] / np.sqrt(Cum_Grad_W['b2'] + eps)\n",
    "    m_t['w_fc'] = m_t['w_fc'] * beta1 + (1 - beta1) * Grad_W['w_fc']\n",
    "    W['w_fc'] -= learning_rate * m_t['w_fc'] / np.sqrt(Cum_Grad_W['w_fc'] + eps)\n",
    "    m_t['b_fc'] = m_t['b_fc'] * beta1 + (1 - beta1) * Grad_W['b_fc']\n",
    "    W['b_fc'] -= learning_rate * m_t['b_fc'] / np.sqrt(Cum_Grad_W['b_fc'] + eps)\n",
    "    \n",
    "    return W\n",
    "\n",
    "def cumulate_grads(Grad_W, Cum_Grad_W):\n",
    "    #beta2 * v_{t-1} + (1 - beta2) * g * g\n",
    "    Cum_Grad_W['w1'] = Cum_Grad_W['w1'] * beta2 + (1 - beta2) * np.square(Grad_W['w1'])\n",
    "    Cum_Grad_W['b1'] = Cum_Grad_W['b1'] * beta2 + (1 - beta2) * np.square(Grad_W['b1'])\n",
    "    Cum_Grad_W['w2'] = Cum_Grad_W['w2'] * beta2 + (1 - beta2) * np.square(Grad_W['w2'])\n",
    "    Cum_Grad_W['b2'] = Cum_Grad_W['b2'] * beta2 + (1 - beta2) * np.square(Grad_W['b2'])\n",
    "    Cum_Grad_W['w_fc'] = Cum_Grad_W['w_fc'] * beta2 + (1 - beta2) * np.square(Grad_W['w_fc'])\n",
    "    Cum_Grad_W['b_fc'] = Cum_Grad_W['b_fc'] * beta2 + (1 - beta2) * np.square(Grad_W['b_fc'])\n",
    "    \n",
    "    return Cum_Grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.24421339233 1.0\n",
      "20 1.22225605867 1.0\n",
      "40 1.47047368276 0.833333333333\n",
      "60 1.64499782177 0.75\n",
      "80 2.00220128324 0.6\n",
      "100 1.94961671498 0.583333333333\n",
      "120 2.52918196495 0.5\n",
      "140 2.25731131472 0.5625\n",
      "160 2.28643029694 0.555555555556\n",
      "180 2.41991771467 0.55\n",
      "200 2.23290860965 0.590909090909\n",
      "220 2.45597894864 0.583333333333\n",
      "240 2.66591179105 0.576923076923\n",
      "260 2.48217714949 0.607142857143\n",
      "280 2.32923288346 0.633333333333\n",
      "300 2.20219503329 0.65625\n",
      "320 2.7805439529 0.647058823529\n",
      "340 2.82765612883 0.638888888889\n",
      "360 3.36136187894 0.631578947368\n",
      "380 3.1996489228 0.65\n",
      "400 3.04732054567 0.666666666667\n",
      "420 3.48814265009 0.659090909091\n",
      "440 4.16096003509 0.630434782609\n",
      "460 4.54192432424 0.625\n",
      "480 5.03090360626 0.62\n",
      "500 4.83748473882 0.634615384615\n",
      "520 4.65843959271 0.648148148148\n",
      "540 5.12953504787 0.642857142857\n",
      "560 4.9526747789 0.655172413793\n",
      "580 5.75253913074 0.65\n",
      "600 6.54516675854 0.645161290323\n",
      "620 6.76130789339 0.640625\n",
      "640 8.33832540504 0.621212121212\n",
      "660 9.10585168265 0.617647058824\n",
      "680 8.84568539822 0.628571428571\n",
      "700 8.59998604553 0.638888888889\n",
      "720 8.89043315645 0.635135135135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keyi/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740 inf 0.631578947368\n",
      "760 inf 0.641025641026\n",
      "780 inf 0.65\n"
     ]
    }
   ],
   "source": [
    "max_intr = 800\n",
    "\n",
    "# cumulative gradients for adamgrads\n",
    "Cum_Grad_W = {'w1': np.zeros([5, 5]), 'b1': np.zeros([24, 24]), \\\n",
    "    'w2': np.zeros([5, 5]), 'b2': np.zeros([24, 24]), \\\n",
    "    'w_fc': np.zeros([144 * 2, 1]), 'b_fc': np.zeros([1, 1])}\n",
    "\n",
    "#print(W['w1'])\n",
    "\n",
    "for i in np.arange(max_intr):\n",
    "    \n",
    "    batch_x = train_data[i*batch_size:(i+1)*batch_size, :, :]\n",
    "    batch_y = train_labels[i*batch_size:(i+1)*batch_size][:, None]\n",
    "    #batch_x = train_data[0:2, :, :]\n",
    "    #batch_y = train_labels[0:2][:, None]\n",
    "    \n",
    "    # Forward profess\n",
    "    h_conv1 = Relu_act(conv2d_np(batch_x, W['w1']) + W['b1'])\n",
    "    h_pool1 = avg_pool(h_conv1)\n",
    "\n",
    "    h_conv2 = Relu_act(conv2d_np(batch_x, W['w2']) + W['b2'])\n",
    "    h_pool2 = avg_pool(h_conv2)\n",
    "\n",
    "    # feed into the fully connected layer\n",
    "    fc = np.hstack((h_pool1.reshape(-1, 144), h_pool2.reshape(-1, 144)))\n",
    "    y_out = np.dot(fc, W['w_fc']) + W['b_fc']\n",
    "    y_pred = sigmoid_act(y_out)\n",
    "    \n",
    "    if i % eval_interval == 0:\n",
    "        ce = compute_ce_sigmoid_loss(y_pred, batch_y)\n",
    "        train_loss = np.vstack((train_loss, ce))\n",
    "        \n",
    "        tr = compute_accuracy(batch_x, batch_y, W)\n",
    "        #print(test_labels.shape)\n",
    "        #te = compute_accuracy(test_data, test_labels[:, None], W)\n",
    "        \n",
    "        train_acc = np.vstack((train_acc, tr))\n",
    "        #test_acc = np.vstack((test_acc, te))\n",
    "        #print(\"Iteration: %d, training accuracy: %g, testing accuracy: %g, loss: %g\" %\n",
    "        #                    (i, tr, te, np.mean(train_loss)))\n",
    "        print(i, np.mean(train_loss), np.mean(train_acc))\n",
    "        \n",
    "    # compute the gradients and learning rate\n",
    "    Grad_W = compute_gradients(W, batch_x, h_conv1, h_conv2, fc, batch_y, y_pred)\n",
    "    Cum_Grad_W = cumulate_grads(Grad_W, Cum_Grad_W)\n",
    "    #print(Cum_Grad_W['w1'].shape)\n",
    "    #print(Cum_Grad_W['b1'].shape)\n",
    "    l_r = learning_rate * np.sqrt(1 - np.power(beta2, i + 1)) / (1 - np.power(beta1, i + 1))\n",
    "    W = train_opts(W, Grad_W, Cum_Grad_W, l_r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(W['w1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(Grad_W['w1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n"
     ]
    }
   ],
   "source": [
    "print(compute_accuracy(test_data, test_labels[:, None], W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(Grad_W['w_fc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
