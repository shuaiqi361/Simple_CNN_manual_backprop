{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the dataset, the dataset originated from MNIST has been curated. The training and testing data still contain 10 digits, but the task is to classify zero from non-zero digits, so the labels are curated as either one for digit 0 or zero for other digits. In order to balance the data for training, the training data contains 2000 digit 0 images, and 2000 images for other digits. The testing images are the first 1000 images from the original evaluation set, note the code maynot work if you feed it with images from other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and normalize\n",
    "data_file = open('zero_nonzero.pkl', 'rb')\n",
    "train_data, train_labels, test_data, test_labels = pickle.load(data_file, encoding='latin1')\n",
    "data_file.close()\n",
    "\n",
    "# normalize the data, note that the testing data has to be manipulated in the same way as the training data\n",
    "train_data = (train_data - np.mean(train_data)) / np.std(train_data)\n",
    "test_data = (test_data - np.mean(train_data)) / np.std(train_data)\n",
    "\n",
    "# some dummy manipulation for later use\n",
    "train_data = np.reshape(train_data, [-1, 28, 28])\n",
    "test_data = np.reshape(test_data, [-1, 28, 28])\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a look at the training and testing data and labels, and an example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 28, 28)\n",
      "(4000, 1)\n",
      "(1000, 28, 28)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  [0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  [1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAB+5JREFUeJzt3X+s1XUdx/Fzf3QVkJwOhcklcE4Xc7PZJopbom1szVISs+b8J1ObMRspbbZaszb+aLP8keUftdVq6B/CBOdqNSrINbFMZy43+gNF+RFMQ+OigMI5/UP/9X2f2z3cw4XX4/Hv636/5yp73u8fn3vuGeh0Oi0gz+CJ/gaAE0P8EEr8EEr8EEr8EEr8EEr8EEr8EEr8EGq4ny+2dPAmv04Ik2xje+3AeL7Okx9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9CiR9C9fUjujn5/PuWK8p9ydefK/fV577QuK1/9+zy2nv/8IVy/+ij+8u9/fLWck/nyQ+hxA+hxA+hxA+hxA+hxA+hxA+hBjqdTt9ebOngTf17sZNIZ/HH6n24/hk99N77zde+8Ep57b4vLS739d+5v9xnD00r971HDzZu73UGymvnD4+UezfXfe72xm1gy996uvdUtrG9tv4fe4wnP4QSP4QSP4QSP4QSP4QSP4QSP4Tyfv7jYHjueeX+6h0Lyv2Pt9Vn6WcNnl7urx051Lit+OJXy2uXfW1TuZ8zdFq5r37rknJ/5t4rG7ehw0fLa2et3l7uv1jw23J//IlHG7fP3n1Pee2MdX8u91OBJz+EEj+EEj+EEj+EEj+EEj+EctR3HHQ7ynvpjoe73KF+6+qXd1xd7rtWXdB8563bu7x2b/76mfPLfWTH8xO+99iNs8v9zg2fLPefzNvcuO2+un7tC9fV+6nAkx9CiR9CiR9CiR9CiR9CiR9CiR9COecfp+H58xq3lZ9/qqd7f3PP5eW+91P1z+iBd15q3Oo3zbZaa9bXZ+Xrd11T7rN2bOnyChN3ZM/ect/2vUX1DX68uXF6YdmD5aVXvL2q3Bd8e/L+u/vFkx9CiR9CiR9CiR9CiR9CiR9CiR9COecfpzcemtm43XbmG12urn/Gbr1xtNyPvtPt/hM3/75nJ+3ek23ahr+U+7KV1zVuT130dHntwqteLffmDx4/eXjyQyjxQyjxQyjxQyjxQyjxQyjxQyjn/OPU6Qw0bu1Wu7x2xY76PfHtN/81oe+JWuf6scZtxe+uKq+9c+7mcv/+NbeU+9CmF8t9KvDkh1Dih1Dih1Dih1Dih1Dih1Dih1DO+Y8ZnlN/FvwDlzwx4Xs/t2t+uY+++8qE702z9ljzOf+WXReX1/5odHO5r7rrcLmPbirnKcGTH0KJH0KJH0KJH0KJH0KJH0I56vuvaaeX85Jp70341mc9fsaEr2VqeuzjPyv3b829odyP7Np9PL+dCfHkh1Dih1Dih1Dih1Dih1Dih1Dih1DO+ftg+p767Z/036yfTq+/4PJ6XjjS5bk5PPT/fUMngCc/hBI/hBI/hBI/hBI/hBI/hBI/hHLOP06DPfycLD7du9VqtVpdZibBhw4cKfde/r1brVarNTD1/1U9+SGU+CGU+CGU+CGU+CGU+CGU+CGUc/5j2m/tK/cVO69q3Lp9nPPBOaeV+4xyZTK8/un6cxrarXZvL9Dp9HZ9H3jyQyjxQyjxQyjxQyjxQyjxQyjxQyjn/Me0x8bKfcuui5vH0freb998oNxnrKuv5/gbuWh/T9ffvO3acm/vfbOn+/eDJz+EEj+EEj+EEj+EEj+EEj+EctTXB1eObi/3nTNnlnu3Y0j+t4PLFjVuay59pLx208EPl/v+++aV+9ChF8t9KvDkh1Dih1Dih1Dih1Dih1Dih1Dih1DO+fug25/2Xn728nJ3zj8xax55oHGbPVT/OfXl624t9ws3PTeh72kq8eSHUOKHUOKHUOKHUOKHUOKHUOKHUM75x2nomTMbt7HL3i+vPXOw/jjoncvr94bPeXBHuZ+qqvfjt1qt1oJvbC33jwyf0bid/5vby2svWnnyn+N348kPocQPocQPocQPocQPocQPocQPoQY6nU7fXmzp4E39e7E++ueGheX+/GW/7On+i+5fWe5zHnq2p/tPpsHiMwn+sbr42PNWq/Xk9Q+X+8KR+tl19+5PNG6v3bqgvLb99/p3CKayje21A+P5Ok9+CCV+CCV+CCV+CCV+CCV+COUtvcfBOT+cXu6v/7x+y+/84ZFy//2q+8v99hua//T3tl9dUF478/V2uXfzwYz6VOnyrzR/VPWG8+qPye72bFp/4Nxy33bZoWI9eY/yjhdPfgglfgglfgglfgglfgglfgglfgjlLb19sO/WxeW+/rv1Of7soWnl3m71dlbfi8Euz49evreFG+6q9x/sLfcjr26f8GufzLylFyiJH0KJH0KJH0KJH0KJH0KJH0I5558ChueNlvuea+uP8N536dHGbdboO+W1f7r0sXLvZu/Rw+W+5Nf3NG7nP1n/DsDIppfLvfNB/XcSUjnnB0rih1Dih1Dih1Dih1Dih1Dih1DO+eEU45wfKIkfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQvX1I7qBqcOTH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0L9B0zbP8hck5AVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the image, note that label for digit 0 is one, and zero for other digits\n",
    "plt.figure(1)\n",
    "img = test_data[0, :, :].reshape(28, 28)\n",
    "print('label: ', test_labels[0, :])\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2)\n",
    "img = test_data[6, :, :].reshape(28, 28)\n",
    "print('label: ', test_labels[6, :])\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's define some hyperparameters for the CNN, and some statistics variables to track the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_train, dim1, dim2 = train_data.shape\n",
    "hm_test = len(test_labels)\n",
    "hm_classes = 2\n",
    "batch_size = 2\n",
    "max_epoch = 1\n",
    "eval_interval = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "test_acc = np.zeros((0, 1))\n",
    "train_acc = np.zeros((0, 1))\n",
    "loss = np.zeros((0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Core functions for the forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights, typically, the weights should be some small random numbers, but since this is a simple demo\n",
    "# we can initialize them in the following way\n",
    "def init_weight(shape):\n",
    "    w = np.ones(shape) / 100\n",
    "    #w = np.abs(np.random.randn(shape[0], shape[1])) / 100\n",
    "    return w\n",
    "\n",
    "\n",
    "def init_bias(shape):\n",
    "    b = np.ones(shape) * 0.001\n",
    "    return b\n",
    "\n",
    "\n",
    "# Sigmoid function and its gradient\n",
    "def sigmoid_act(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "def sigmoid_grad(x):\n",
    "    return sigmoid_act(x) * (1 - sigmoid_act(x))\n",
    "\n",
    "\n",
    "# Relu activation function and its gradient\n",
    "def Relu_act(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "\n",
    "def Relu_grad(x):\n",
    "    return (x > 0) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve the image with a given filter\n",
    "def conv2d_np(images, filt2d):\n",
    "    hm_im, d1, d2 = images.shape\n",
    "    \n",
    "    # default stride 1 and square filter with size 5, in our particular example\n",
    "    stride = 1\n",
    "    filter_size = 5\n",
    "    conv_im = np.zeros([hm_im, int((d1-filter_size)/stride + 1), int((d2-filter_size)/stride + 1)])\n",
    "    \n",
    "    # convolve with the input images\n",
    "    for im in np.arange(int(hm_im)):\n",
    "        for i in np.arange(int((d1-filter_size+1)/stride)):\n",
    "            for j in np.arange(int((d2-filter_size+1)/stride)):\n",
    "                # convolution is done by the element-wise multiplication between the filter and the reception area\n",
    "                # the result is the sum of this matrix\n",
    "                recep_area = images[im, i:i+filter_size, j:j+filter_size]\n",
    "                conv_im[im, i, j] = np.sum(recep_area * filt2d)\n",
    "                \n",
    "    return conv_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different pooling scenarios, both are commonly used\n",
    "def avg_pool3(images):\n",
    "    hm_im, d1, d2 = images.shape\n",
    "    # by default, the stride is 3, and the kernel is 3 x 3 in our example\n",
    "    stride = 3\n",
    "    kernel_size = 3\n",
    "    pooled_im = np.zeros([hm_im, int((d1-kernel_size)/stride + 1), int((d2-kernel_size)/stride + 1)])\n",
    "    for im in np.arange(int(hm_im)):\n",
    "        for i in np.arange(int((d1-kernel_size)/stride + 1)):\n",
    "            for j in np.arange(int((d2-kernel_size)/stride + 1)):\n",
    "                # average the values in the reception area and preduce one output\n",
    "                recep_area = images[im, i * stride:i * stride + kernel_size, j * stride:j * stride + kernel_size]\n",
    "                pooled_im[im, i, j] = np.mean(recep_area)\n",
    "                \n",
    "    return pooled_im\n",
    "\n",
    "def max_pool3(images):\n",
    "    hm_im, d1, d2 = images.shape\n",
    "    # by default, the stride is 3, and the kernel is 3 x 3 in our example\n",
    "    stride = 3\n",
    "    kernel_size = 3\n",
    "    pooled_im = np.zeros([hm_im, int((d1-kernel_size)/stride + 1), int((d2-kernel_size)/stride + 1)])\n",
    "    ind_set = np.zeros([hm_im, 64, 2])\n",
    "    \n",
    "    for im in np.arange(int(hm_im)):\n",
    "        counter = 0\n",
    "        for i in np.arange(int((d1-kernel_size)/stride + 1)):\n",
    "            for j in np.arange(int((d2-kernel_size)/stride + 1)):\n",
    "                recep_area = images[im, i * stride:i * stride + kernel_size, j * stride:j * stride + kernel_size]\n",
    "                \n",
    "                # keep track of the indices for the fired value in each reception area, \n",
    "                # we will use this index in later phase when we compute the gradients\n",
    "                ind = np.unravel_index(np.argmax(recep_area, axis=None), recep_area.shape)\n",
    "                pooled_im[im, i, j] = recep_area[ind]\n",
    "                ind_set[im, counter, :] = ind\n",
    "                counter += 1\n",
    "                \n",
    "    return pooled_im, ind_set.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the prediction accuracy\n",
    "def compute_accuracy(data, labels, W, ind_set, pool='AVERAGE'):\n",
    "    h_conv1 = Relu_act(conv2d_np(data, W['w']) + W['b'])\n",
    "    if pool == 'AVERAGE':\n",
    "        h_pool1 = avg_pool3(h_conv1)\n",
    "    else:\n",
    "        h_pool1, _ = max_pool3(h_conv1)\n",
    "\n",
    "    fc = h_pool1.reshape(-1, 64)\n",
    "    y_out = np.dot(fc, W['w_fc']) + W['b_fc']\n",
    "    \n",
    "    # if the value is greater than zero, we label it as one\n",
    "    pred_labels = ((y_out > 0) * 1.0).reshape(len(labels), 1)\n",
    "    \n",
    "    return np.mean(np.equal(pred_labels, labels) * 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid cross entropy loss function\n",
    "def compute_ce_sigmoid_loss(y_out, y):\n",
    "    # note that the y_out is the unfired value, namely, the value before the sigmoid function,\n",
    "    # this implementation will avoid underflow, since the log is involved\n",
    "    return np.sum(y_out - y_out * y + np.log(1 + np.exp(-y_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the gradients!\n",
    "# This is the key for training a CNN, but it is pretty straightforward\n",
    "# Let's break it down piece by piece\n",
    "def compute_gradients(W, x, h_conv1, fc, y, y_hat, ind_set, pool='AVERAGE'):\n",
    "    # initialize the gradients to return\n",
    "    Grad_W = {'w': np.zeros([5, 5]), 'b': np.zeros([24, 24]), \\\n",
    "        'w_fc': np.zeros([64, 1]), 'b_fc': np.zeros([1, 1])}\n",
    "    \n",
    "    # Delta_y_hat = -y * 1 / y_hat + (1-y) * 1 / (1 - y_hat), simple\n",
    "    # Dy_hat / D_w = y_hat(1 - y_hat) * h\n",
    "    Grad_W['w_fc'] = np.dot(np.transpose(fc), y_hat - y)\n",
    "    Grad_W['b_fc'] = np.sum(y_hat - y)\n",
    "    Grad_fc = np.dot(y_hat - y, np.transpose(W['w_fc']))\n",
    "    \n",
    "    stride = 3\n",
    "    act1 = np.zeros([24, 24])\n",
    "    for im in np.arange(len(y)):\n",
    "        one_im = Grad_fc[im, :]\n",
    "        pool1 = one_im.reshape(8, 8)\n",
    "\n",
    "        counter = 0\n",
    "        for i in np.arange(8):\n",
    "            for j in np.arange(8):\n",
    "                if pool == 'AVERAGE':\n",
    "                    # the gradient will be uniformly distributed back to the original reception area\n",
    "                    act1[i*stride:i*stride+stride, j*stride:j*stride+stride] = \\\n",
    "                        np.ones([stride, stride]) * pool1[i, j] / (stride * stride)\n",
    "                else:\n",
    "                    temp_map = np.zeros([stride, stride])\n",
    "                    temp_map[ind_set[im, counter, :]] = pool1[i, j]\n",
    "                    # only the entry in the index set will take the full gradient, other entries would be zero\n",
    "                    act1[i*stride:i*stride+stride, j*stride:j*stride+stride] = temp_map\n",
    "                        \n",
    "        # Gradient for the Relu   \n",
    "        Grad_C1 = (h_conv1[im, :, :].reshape(24, 24) > 0) * act1\n",
    "        \n",
    "        # Gradient for the bias\n",
    "        Grad_W['b'] += Grad_C1\n",
    "        \n",
    "        f1 = np.zeros([5, 5])\n",
    "        for r in np.arange(5):\n",
    "            for c in np.arange(5):\n",
    "                f1[r, c] = np.sum(x[im, r:r+24, c:c+24].reshape(24, 24) * Grad_C1)\n",
    "        \n",
    "        # Gradient for the filter2d\n",
    "        Grad_W['w'] += f1\n",
    "        \n",
    "    return Grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the standard stachatic gradient descent\n",
    "def SGDtrain_opts(W, Grad_W, learning_rate):\n",
    "    W['w'] -= learning_rate * Grad_W['w']\n",
    "    W['b'] -= learning_rate * Grad_W['b']\n",
    "    W['w_fc'] -= learning_rate * Grad_W['w_fc']\n",
    "    W['b_fc'] -= learning_rate * Grad_W['b_fc']\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we start to train the simple CNN, I will print the loss and accuracy every 100 iterations, after one epoch's training, the testing accuracy will hit over 98%, the CNN is indeed powerful, don't forget that we only have one filter(5*5=25 parameters), one bias(24*24=576 parameters), weights between FC layer and the output layer(64+1=65 parameters), totally 666 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 28, 28) (2, 1)\n",
      "Epoch 0 iteration 0 train loss: 1.366501825826 test_acc: 0.096\n",
      "Epoch 0 iteration 50 train loss: 1.2592087002731813 test_acc: 0.096\n",
      "Epoch 0 iteration 100 train loss: 1.1592556243063454 test_acc: 0.096\n",
      "Epoch 0 iteration 150 train loss: 1.068714264936131 test_acc: 0.163\n",
      "Epoch 0 iteration 200 train loss: 1.0057041539727467 test_acc: 0.58\n",
      "Epoch 0 iteration 250 train loss: 1.081024854408825 test_acc: 0.804\n",
      "Epoch 0 iteration 300 train loss: 1.029127398064196 test_acc: 0.932\n",
      "Epoch 0 iteration 350 train loss: 0.9757851209113928 test_acc: 0.876\n",
      "Epoch 0 iteration 400 train loss: 0.932587821354404 test_acc: 0.926\n",
      "Epoch 0 iteration 450 train loss: 0.8431732266027228 test_acc: 0.953\n",
      "Epoch 0 iteration 500 train loss: 0.7868714347719752 test_acc: 0.978\n",
      "Epoch 0 iteration 550 train loss: 0.7218627827940672 test_acc: 0.975\n",
      "Epoch 0 iteration 600 train loss: 0.6713104574257083 test_acc: 0.913\n",
      "Epoch 0 iteration 650 train loss: 0.6240010718360945 test_acc: 0.974\n",
      "Epoch 0 iteration 700 train loss: 0.6212222443171761 test_acc: 0.981\n",
      "Epoch 0 iteration 750 train loss: 0.5839217084551518 test_acc: 0.936\n",
      "Epoch 0 iteration 800 train loss: 0.5819912949878046 test_acc: 0.973\n",
      "Epoch 0 iteration 850 train loss: 0.5588411782031901 test_acc: 0.971\n",
      "Epoch 0 iteration 900 train loss: 0.5380464463445142 test_acc: 0.973\n",
      "Epoch 0 iteration 950 train loss: 0.5432407249324951 test_acc: 0.942\n",
      "Epoch 0 iteration 1000 train loss: 0.5231517040210133 test_acc: 0.96\n",
      "Epoch 0 iteration 1050 train loss: 0.5671829525465157 test_acc: 0.981\n",
      "Epoch 0 iteration 1100 train loss: 0.5616721426561534 test_acc: 0.985\n",
      "Epoch 0 iteration 1150 train loss: 0.5418435973889671 test_acc: 0.974\n",
      "Epoch 0 iteration 1200 train loss: 0.5264215569954847 test_acc: 0.972\n",
      "Epoch 0 iteration 1250 train loss: 0.5072547575324138 test_acc: 0.972\n",
      "Epoch 0 iteration 1300 train loss: 0.48991923635783763 test_acc: 0.977\n",
      "Epoch 0 iteration 1350 train loss: 0.4788378229502427 test_acc: 0.984\n",
      "Epoch 0 iteration 1400 train loss: 0.4623586723028387 test_acc: 0.965\n",
      "Epoch 0 iteration 1450 train loss: 0.4472815984160728 test_acc: 0.987\n",
      "Epoch 0 iteration 1500 train loss: 0.4687182004066499 test_acc: 0.957\n",
      "Epoch 0 iteration 1550 train loss: 0.4542085853504646 test_acc: 0.983\n",
      "Epoch 0 iteration 1600 train loss: 0.44356398488636617 test_acc: 0.973\n",
      "Epoch 0 iteration 1650 train loss: 0.4391671111009216 test_acc: 0.987\n",
      "Epoch 0 iteration 1700 train loss: 0.4274069924747396 test_acc: 0.971\n",
      "Epoch 0 iteration 1750 train loss: 0.4157034187268241 test_acc: 0.987\n",
      "Epoch 0 iteration 1800 train loss: 0.40508076165277557 test_acc: 0.984\n",
      "Epoch 0 iteration 1850 train loss: 0.39460770312405075 test_acc: 0.981\n",
      "Epoch 0 iteration 1900 train loss: 0.3854924822336867 test_acc: 0.984\n",
      "Epoch 0 iteration 1950 train loss: 0.3759968756417679 test_acc: 0.982\n"
     ]
    }
   ],
   "source": [
    "# Initialize the weights and index_set, note that the index set size is not fixed, it depends on the batch_size\n",
    "W = {'w': init_weight([5, 5]), 'b': init_bias([24, 24]), \\\n",
    "                'w_fc': init_weight([64, 1]), 'b_fc': init_bias([1, 1])}\n",
    "ind_set = np.zeros([batch_size, 64, 2])\n",
    "\n",
    "for epoch in np.arange(max_epoch):\n",
    "        # reshuffle the data after each epoch\n",
    "        re_order = np.random.permutation(hm_train)\n",
    "        \n",
    "        for i in np.arange(int(hm_train / batch_size)):\n",
    "            batch_x = train_data[re_order[i * batch_size:(i + 1) * batch_size], :, :]\n",
    "            batch_y = train_labels[re_order[i * batch_size:(i + 1) * batch_size], :]\n",
    "            if epoch == 0 and i == 0:\n",
    "                print(batch_x.shape, batch_y.shape)\n",
    "            \n",
    "            # Forward propagation\n",
    "            h_conv1 = Relu_act(conv2d_np(batch_x, W['w']) + W['b'])\n",
    "            #h_pool1, ind_set = avg_pool3(h_conv1)\n",
    "            h_pool1, ind_set = max_pool3(h_conv1)\n",
    "\n",
    "            # feed into the fully connected layer\n",
    "            fc = h_pool1.reshape(-1, 64)\n",
    "            y_out = np.dot(fc, W['w_fc']) + W['b_fc']\n",
    "            y_pred = sigmoid_act(y_out)\n",
    "                        \n",
    "            if i % eval_interval == 0:\n",
    "                ce = compute_ce_sigmoid_loss(y_out, batch_y)\n",
    "                test_accuracy = compute_accuracy(test_data, test_labels, W, ind_set, 'MAX')\n",
    "                #test_accuracy = compute_accuracy(test_data, test_labels, W, ind_set, 'AVERAGE')\n",
    "                #train_accuracy = compute_accuracy(train_data, train_labels, W, ind_set, 'MAX')\n",
    "                test_acc = np.vstack((test_acc, test_accuracy))\n",
    "                #train_acc = np.vstack((train_acc, train_accuracy))\n",
    "                loss = np.vstack((loss, ce))\n",
    "                print('Epoch', epoch, 'iteration', i, 'train loss:', np.mean(loss), 'test_acc:', test_accuracy)\n",
    "                \n",
    "            \n",
    "            # Backward propagation\n",
    "            Grad_W = compute_gradients(W, batch_x, h_conv1, fc, batch_y, y_pred, ind_set, 'MAX')\n",
    "            W = SGDtrain_opts(W, Grad_W, learning_rate)\n",
    "            \n",
    "            # decay the learning rate if necessary\n",
    "            if i == int(hm_train / batch_size / 2):\n",
    "                learning_rate *= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
